{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb78a69440345fa8c3f281853882a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b51274ead2f402bbd4d64406481a3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f814ad8bd6469b901cbb038e9c9338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc81b12ef7f4f658f2fc3242c50bdc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74059d112fe640c6b2ad9cd07b261495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c97b6718ff64b8f9e66395374cc64f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bb711f9dd349d1b82ad258a0429af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5694d25125c3436eb84df6845cc9d45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2AudioForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info  \n",
    "\n",
    "model_vl = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"cuda\"\n",
    ")\n",
    "processor_vl = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "processor_audio = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model_audio = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-Audio-7B-Instruct\", device_map=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"C:\\\\Users\\\\prath\\\\Desktop\\\\Confidence\\\\video_sample.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS: 30.0, Total frames: 609, Duration: 20.3s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "duration = frame_count / fps if fps>0 else None\n",
    "print(f\"Video FPS: {fps}, Total frames: {frame_count}, Duration: {duration}s\")\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:2504: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "messages_vl = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"video\",\n",
    "                \"video\": \"C:\\\\Users\\\\prath\\Desktop\\\\Confidence\\\\video_sample.mp4\",\n",
    "                \"fps\": 1.0  \n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \" Based on the facial expression of this video, evaluate the confidence level of the person and return the output as only one label out of the following three: Low Confidence, Moderately Confident, Highly Confident.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "text_prompt = processor_vl.apply_chat_template(messages_vl, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = process_vision_info(messages_vl)\n",
    "inputs_vl = processor_vl(text=[text_prompt], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "inputs_vl = inputs_vl.to(\"cuda\")\n",
    "\n",
    "gen_ids_vl = model_vl.generate(**inputs_vl, max_new_tokens=150)\n",
    "gen_ids_vl = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs_vl.input_ids, gen_ids_vl)]\n",
    "output_text_vl = processor_vl.batch_decode(gen_ids_vl, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(\"Vision analysis:\", output_text_vl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "clip = VideoFileClip(video_path)\n",
    "audio_path = \"temp_audio.wav\"\n",
    "clip.audio.write_audiofile(audio_path, logger=None)  \n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"audio\", \"audio_url\": f\"file://{audio_path}\"},\n",
    "        {\"type\": \"text\", \"text\": \"Based on this audio evaluate the confidence level of the person and return the output as only one label out of the following three: Low Confidence, Moderately Confident, Highly Confident. \"}\n",
    "    ]}\n",
    "]\n",
    "\n",
    "text_audio = processor_audio.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "import librosa\n",
    "audio_wave, _ = librosa.load(audio_path, sr=processor_audio.feature_extractor.sampling_rate)\n",
    "inputs_audio = processor_audio(text=[text_audio], audios=[audio_wave], return_tensors=\"pt\", padding=True)\n",
    "inputs_audio = inputs_audio.to(\"cuda\")\n",
    "\n",
    "gen_ids_audio = model_audio.generate(**inputs_audio, max_length=150)\n",
    "\n",
    "gen_ids_audio = gen_ids_audio[:, inputs_audio.input_ids.size(1):]\n",
    "output_text_audio = processor_audio.batch_decode(gen_ids_audio, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(\"Audio analysis:\", output_text_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Final confidence assessment (vision + audio):\")\n",
    "print(\"- From video (facial cues):\", output_text_vl)\n",
    "print(\"- From audio (vocal cues):\", output_text_audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'hf_cache_home' from 'transformers.utils' (c:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m cached_file, hf_cache_home\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mshutil\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mos\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'hf_cache_home' from 'transformers.utils' (c:\\Users\\prath\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers.utils import cached_file, hf_cache_home\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# List of model names to delete\n",
    "models = [\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
    "]\n",
    "\n",
    "# Hugging Face cache directory\n",
    "cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\", \"transformers\")\n",
    "\n",
    "# Delete model folders manually\n",
    "for model_name in models:\n",
    "    model_dir = os.path.join(cache_dir, model_name.replace(\"/\", \"-\"))\n",
    "    if os.path.exists(model_dir):\n",
    "        shutil.rmtree(model_dir)\n",
    "        print(f\"Deleted: {model_dir}\")\n",
    "    else:\n",
    "        print(f\"Not found: {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
